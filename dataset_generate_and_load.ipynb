{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\Chen Qun\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# 输入你的 Hugging Face API token\n",
    "login('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')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2010年12月"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 59.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from  tqdm import tqdm\n",
    "\n",
    "dataset_list = []\n",
    "# key: year month date page text\n",
    "\n",
    "# 2012.12\n",
    "datapath = \"C:/Users/Chen Qun/Desktop/各年文件/09、10、19/2010/12/\"\n",
    "# year = 2010\n",
    "# month = 12\n",
    "for date in tqdm(range(1,31+1)):\n",
    "    for page in range(1,30+1):\n",
    "        onepage = datapath\n",
    "        if date<10:\n",
    "            onepage += '0'\n",
    "        onepage += str(date)\n",
    "        onepage += '/2010_12_'\n",
    "        if date<10:\n",
    "            onepage += '0'\n",
    "        onepage += str(date)\n",
    "        onepage += \"_\" + str(page) + \".txt\"\n",
    "        #print(onepage)\n",
    "        if os.path.exists(onepage):\n",
    "            with open(onepage,'r',encoding='utf-8') as f:\n",
    "                raw_text = f.read()\n",
    "                #print(len(raw_text))\n",
    "                dataset_list.append({\n",
    "                    \"year\":2010,\n",
    "                    \"month\":12,\n",
    "                    \"date\":date,\n",
    "                    \"page\":page,\n",
    "                    \"text\":raw_text\n",
    "                })\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:03<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from  tqdm import tqdm\n",
    "\n",
    "dataset_list_2011 = []\n",
    "# key: year month date page text\n",
    "\n",
    "# 2012.12\n",
    "datapath = \"C:/Users/Chen Qun/Desktop/各年文件/2011/2011/\"\n",
    "# year = 2011\n",
    "for month in tqdm(range(1,12+1)):\n",
    "    for date in (range(1,31+1)):\n",
    "        for page in range(1,30+1):\n",
    "            onepage = datapath\n",
    "            if month <10:\n",
    "                onepage += '0'\n",
    "            onepage += str(month)+'/'\n",
    "            if date<10:\n",
    "                onepage += '0'\n",
    "            onepage += str(date)\n",
    "            onepage += '/2011_'\n",
    "            if month <10:\n",
    "                onepage += '0'\n",
    "            onepage += str(month) + \"_\"\n",
    "            if date<10:\n",
    "                onepage += '0'\n",
    "            onepage += str(date)\n",
    "            onepage += \"_\" + str(page) + \".txt\"\n",
    "            #print(onepage)\n",
    "            if os.path.exists(onepage):\n",
    "                with open(onepage,'r',encoding='utf-8') as f:\n",
    "                    raw_text = f.read()\n",
    "                    #print(len(raw_text))\n",
    "                    dataset_list_2011.append({\n",
    "                        \"year\":2011,\n",
    "                        \"month\":month,\n",
    "                        \"date\":date,\n",
    "                        \"page\":page,\n",
    "                        \"text\":raw_text\n",
    "                    })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list.extend(dataset_list_2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69041"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 0\n",
    "for item in dataset_list:\n",
    "    maxlen = max(maxlen, len(item['text']))\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_cnt = 0\n",
    "for item in dataset_list:\n",
    "    if len(item['text']) <= 50:\n",
    "        zero_cnt+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9595c83abc824f4e9e34e796dfa127e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77194786e7ce4e5d844460db67e469ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5a1e6f022f430aaca86ae2160aa63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_list(dataset_list)\n",
    "\n",
    "ds.push_to_hub(\"Al-Ahram-raw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typo\n",
    "\n",
    "似乎单行数据集太大网页炸了\n",
    "\n",
    "切割一下（"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10852/10852 [00:00<00:00, 22179.50it/s]\n"
     ]
    }
   ],
   "source": [
    "sliced_text = []\n",
    "batchsize = 5000\n",
    "for item in tqdm(dataset_list):\n",
    "    idx = 0\n",
    "    while idx < len(item['text']):\n",
    "        sliced_text.append({\n",
    "            'year':item['year'],\n",
    "            'month':item['month'],\n",
    "            'date':item['date'],\n",
    "            'page':item['page'],\n",
    "            'slice':idx // batchsize + 1,\n",
    "            'text':item['text'][idx:idx+batchsize]\n",
    "        })\n",
    "        idx += batchsize\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6bb039575149e0b267508b1fb492ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df7ab99023045dea551e22584f14294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/52 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34ab6acf8af473e83ebddb0ef965af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_list(sliced_text)\n",
    "\n",
    "ds.push_to_hub(\"Al-Ahram-raw-sliced\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
